apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-code
  namespace: mlops-light
data:
  serve.py: |
    from flask import Flask, request, jsonify
    import torch
    import torchvision
    import torchvision.transforms as transforms
    from PIL import Image
    import io
    from minio import Minio
    
    app = Flask(__name__)
    
    CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
    
    transform = transforms.Compose([
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    def load_model():
        client = Minio("minio.mlops-light.svc.cluster.local:9000",
                      access_key="minioadmin", secret_key="minioadmin", secure=False)
        
        client.fget_object("models", "resnet-cifar10/model_latest.pth", "/tmp/model.pth")
        
        model = torchvision.models.resnet18(weights=None)
        model.fc = torch.nn.Linear(model.fc.in_features, 10)
        
        checkpoint = torch.load("/tmp/model.pth", map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        return model
    
    model = load_model()
    
    @app.route('/predict', methods=['POST'])
    def predict():
        file = request.files['file']
        img = Image.open(io.BytesIO(file.read())).convert('RGB')
        img_tensor = transform(img).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(img_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            conf, pred = probs.max(1)
        
        return jsonify({
            "prediction": CLASSES[pred.item()],
            "confidence": float(conf.item())
        })
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resnet-inference
  namespace: mlops-light
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference
  template:
    metadata:
      labels:
        app: inference
    spec:
      containers:
      - name: api
        image: python:3.9
        command:
        - bash
        - -c
        - |
          pip install flask torch torchvision minio pillow
          python /app/serve.py
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: code
          mountPath: /app
        resources:
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: code
        configMap:
          name: inference-code
---
apiVersion: v1
kind: Service
metadata:
  name: inference
  namespace: mlops-light
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    app: inference
