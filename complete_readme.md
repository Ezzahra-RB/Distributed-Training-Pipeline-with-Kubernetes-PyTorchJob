# ğŸš€ Distributed Deep Learning Pipeline with Kubernetes & PyTorch

[![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=flat&logo=kubernetes&logoColor=white)](https://kubernetes.io/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![MinIO](https://img.shields.io/badge/MinIO-C72E49?style=flat&logo=minio&logoColor=white)](https://min.io/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

> Pipeline MLOps end-to-end pour l'entraÃ®nement distribuÃ© de ResNet-18 sur CIFAR-10 avec Kubernetes, PyTorchJob et MinIO.

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'Ensemble](#-vue-densemble)
- [Architecture](#-architecture)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [PrÃ©requis](#-prÃ©requis)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Pipeline MLOps](#-pipeline-mlops)
- [RÃ©sultats](#-rÃ©sultats)
- [Monitoring](#-monitoring)
- [DÃ©ploiement](#-dÃ©ploiement)
- [Troubleshooting](#-troubleshooting)
- [Contributeurs](#-contributeurs)

---

## ğŸ¯ Vue d'Ensemble

Ce projet implÃ©mente un **pipeline MLOps complet** pour l'entraÃ®nement distribuÃ© de rÃ©seaux de neurones profonds sur Kubernetes. Il dÃ©montre les meilleures pratiques de l'industrie pour le Machine Learning en production :

- âœ… **Distributed Training** : EntraÃ®nement parallÃ¨le avec PyTorch DDP
- âœ… **Container Orchestration** : DÃ©ploiement et scaling avec Kubernetes
- âœ… **Artifact Management** : Versioning des modÃ¨les avec MinIO (S3-compatible)
- âœ… **Reproducibility** : Pipeline automatisÃ© et reproductible
- âœ… **Production-Ready** : Architecture scalable et dÃ©ployable

### ğŸ“ Cas d'Usage

- EntraÃ®nement de modÃ¨les de vision par ordinateur Ã  grande Ã©chelle
- MLOps dans des environnements Ã  ressources limitÃ©es
- Prototype de pipeline de production pour startups/PME
- Projet acadÃ©mique de Deep Learning distribuÃ©

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Kubernetes Cluster (Minikube)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚          MinIO               â”‚          â”‚
â”‚  â”‚    (S3-Compatible Storage)   â”‚          â”‚
â”‚  â”‚                              â”‚          â”‚
â”‚  â”‚  Buckets:                    â”‚          â”‚
â”‚  â”‚  â€¢ datasets/  (CIFAR-10)     â”‚          â”‚
â”‚  â”‚  â€¢ models/    (Checkpoints)  â”‚          â”‚
â”‚  â”‚  â€¢ metrics/   (JSON logs)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                  â†‘                          â”‚
â”‚                  â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚      PyTorchJob (DDP)          â”‚        â”‚
â”‚  â”‚                                 â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚  â”‚  Master  â”‚    â”‚  Worker  â”‚  â”‚        â”‚
â”‚  â”‚  â”‚  (Rank 0)â”‚â—„â”€â”€â–ºâ”‚  (Rank 1)â”‚  â”‚        â”‚
â”‚  â”‚  â”‚          â”‚    â”‚          â”‚  â”‚        â”‚
â”‚  â”‚  â”‚ ResNet-18â”‚    â”‚ ResNet-18â”‚  â”‚        â”‚
â”‚  â”‚  â”‚ Training â”‚    â”‚ Training â”‚  â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”‚                                 â”‚        â”‚
â”‚  â”‚  Communication: TCP (gloo)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Technologies UtilisÃ©es

| Composant | Technologie | Version | RÃ´le |
|-----------|------------|---------|------|
| **Orchestration** | Kubernetes | 1.34+ | Gestion des conteneurs |
| **Cluster** | Minikube | 1.37+ | Cluster Kubernetes local |
| **Training Framework** | PyTorch | 2.0.0 | Deep Learning |
| **Distributed Training** | PyTorch DDP | - | ParallÃ©lisation |
| **Job Operator** | Kubeflow Training Operator | 1.8.1 | Gestion PyTorchJob |
| **Storage** | MinIO | 2023-09 | Object storage (S3) |
| **Dataset** | CIFAR-10 | - | 60K images 32x32 |
| **Model** | ResNet-18 | - | CNN (11M params) |

---

## âœ¨ FonctionnalitÃ©s

### ğŸ¯ Pipeline MLOps Complet

1. **Data Ingestion**
   - TÃ©lÃ©chargement automatique de CIFAR-10
   - Sauvegarde dans MinIO pour rÃ©utilisation
   - Support de datasets personnalisÃ©s

2. **Feature Engineering**
   - Data augmentation (flip, crop)
   - Normalisation selon statistiques CIFAR-10
   - Distribution intelligente des donnÃ©es entre workers

3. **Distributed Training**
   - PyTorch Distributed Data Parallel (DDP)
   - Communication backend : Gloo (CPU-optimized)
   - Synchronisation automatique des gradients
   - 1 Master + 1 Worker (extensible Ã  N workers)

4. **Model Evaluation**
   - MÃ©triques calculÃ©es Ã  chaque epoch
   - Train accuracy, Test accuracy
   - Train loss, Test loss

5. **Model Versioning**
   - Checkpoints sauvegardÃ©s Ã  chaque epoch
   - Versioning avec timestamps
   - ModÃ¨le "latest" toujours disponible

6. **Automated Deployment**
   - ModÃ¨les prÃªts pour dÃ©ploiement
   - API REST pour infÃ©rence (optionnel)

### ğŸš€ Optimisations

- **EfficacitÃ© mÃ©moire** : Batch size optimisÃ©, gradient accumulation
- **Vitesse** : DataLoader multi-threaded, pin_memory
- **Robustesse** : Gestion d'erreurs, retry automatique
- **ObservabilitÃ©** : Logs dÃ©taillÃ©s, mÃ©triques structurÃ©es

---

## ğŸ“¦ PrÃ©requis

### SystÃ¨me d'Exploitation

- **Windows** : Windows 10/11 avec WSL2
- **Linux** : Ubuntu 20.04+ ou Ã©quivalent
- **macOS** : macOS 11+ avec Docker Desktop

### Ressources MatÃ©rielles

| Composant | Minimum | RecommandÃ© |
|-----------|---------|------------|
| **CPU** | 2 cores | 4 cores |
| **RAM** | 6 GB | 8 GB |
| **Disque** | 20 GB | 30 GB |
| **GPU** | Optionnel | Optionnel |

### Logiciels

- Docker 20.10+
- Kubernetes (via Minikube)
- Python 3.9+
- Git

---

## ğŸ”§ Installation

### Ã‰tape 1 : Environnement de Base

#### Sur Windows (WSL2)

```powershell
# Dans PowerShell en Administrateur
wsl --install
# RedÃ©marrer Windows
```

Puis dans WSL2 Ubuntu :

```bash
# Mise Ã  jour systÃ¨me
sudo apt update && sudo apt upgrade -y

# Installation Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# Fermer et rouvrir WSL pour appliquer les changements
exit
# Puis rouvrir WSL
```

#### Sur Linux

```bash
# Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
newgrp docker

# VÃ©rifier
docker --version
```

### Ã‰tape 2 : Kubernetes (Minikube)

```bash
# TÃ©lÃ©charger Minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Installer kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install kubectl /usr/local/bin/

# VÃ©rifier les installations
minikube version
kubectl version --client
```

### Ã‰tape 3 : DÃ©marrage du Cluster

```bash
# CrÃ©er un cluster Kubernetes local
minikube start --cpus=2 --memory=6144 --disk-size=20g --driver=docker

# VÃ©rifier que le cluster fonctionne
kubectl cluster-info
kubectl get nodes
```

**Sortie attendue** :
```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   1m    v1.34.0
```

### Ã‰tape 4 : Kubeflow Training Operator

```bash
# Cloner le repository
git clone --depth 1 --branch v1.8.1 https://github.com/kubeflow/training-operator.git
cd training-operator

# Installer l'opÃ©rateur
kubectl apply -k manifests/overlays/standalone

# Attendre que ce soit prÃªt
kubectl wait --for=condition=ready pod -l app=training-operator -n kubeflow --timeout=300s

# VÃ©rifier
kubectl get pods -n kubeflow
```

**Sortie attendue** :
```
NAME                                 READY   STATUS    RESTARTS   AGE
training-operator-xxxxx-xxxxx        1/1     Running   0          1m
```

### Ã‰tape 5 : DÃ©pendances Python

```bash
# CrÃ©er un environnement virtuel
cd ~
mkdir mlops-distributed-project
cd mlops-distributed-project

python3 -m venv venv
source venv/bin/activate

# Installer les packages
pip install --upgrade pip
pip install torch torchvision minio requests pillow
```

---

## ğŸš€ Utilisation

### Cloner le Projet

```bash
cd ~/mlops-distributed-project

# Si vous avez un repository Git
git clone <votre-repo-url> .

# OU crÃ©er les fichiers manuellement (voir ci-dessous)
```

### CrÃ©er les Fichiers de Configuration

#### Fichier 1 : `lightweight-pipeline.yaml`

```bash
nano lightweight-pipeline.yaml
```

Copiez le contenu complet du pipeline (voir [lightweight-pipeline.yaml](./lightweight-pipeline.yaml)).

**Points clÃ©s du fichier** :
- Namespace `mlops-light`
- DÃ©ploiement MinIO
- ConfigMap avec le code d'entraÃ®nement
- PyTorchJob avec Master + Worker
- Ressources CPU/MÃ©moire optimisÃ©es

#### Fichier 2 : `quick-run.sh`

```bash
nano quick-run.sh
chmod +x quick-run.sh
```

Copiez le contenu du script de lancement (voir [quick-run.sh](./quick-run.sh)).

### Lancer le Pipeline

```bash
# Tout en une commande
./quick-run.sh
```

**Ce script va** :
1. âœ… Nettoyer les namespaces prÃ©cÃ©dents
2. âœ… DÃ©ployer MinIO
3. âœ… CrÃ©er les buckets nÃ©cessaires
4. âœ… Lancer le PyTorchJob distribuÃ©
5. âœ… Afficher les logs en temps rÃ©el

### Sortie Attendue

```
==========================================
  Version ALLÃ‰GÃ‰E - Sans MLflow
  Uniquement: MinIO + PyTorchJob
==========================================

[1/5] Nettoyage...
namespace "mlops-light" deleted

[2/5] DÃ©ploiement de l'infrastructure...
namespace/mlops-light created
deployment.apps/minio created
service/minio created
pytorchjob.kubeflow.org/resnet-light created

[3/5] Attente de MinIO...
pod/minio-xxxxx condition met

[4/5] Configuration des buckets MinIO...
âœ“ Bucket 'datasets' crÃ©Ã©
âœ“ Bucket 'models' crÃ©Ã©
âœ“ Bucket 'metrics' crÃ©Ã©

[5/5] Lancement de l'entraÃ®nement distribuÃ©...
Logs en temps rÃ©el:
[Rank 0/2] ğŸš€ DÃ‰MARRAGE ENTRAÃNEMENT DISTRIBUÃ‰
...
```

**DurÃ©e totale** : ~25-35 minutes
- Setup : 2-3 min
- EntraÃ®nement : 20-30 min

---

## ğŸ“Š Pipeline MLOps

Le pipeline est composÃ© de **5 Ã©tapes principales** :

### 1ï¸âƒ£ Data Ingestion

```python
# TÃ©lÃ©chargement automatique de CIFAR-10
trainset = torchvision.datasets.CIFAR10(
    root='/data', train=True, download=True, transform=transform_train
)
testset = torchvision.datasets.CIFAR10(
    root='/data', train=False, download=True, transform=transform_test
)
```

**RÃ©sultat** : 50,000 images d'entraÃ®nement + 10,000 images de test

### 2ï¸âƒ£ Feature Engineering

```python
# Transformations et augmentation
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),      # Flip horizontal alÃ©atoire
    transforms.RandomCrop(32, padding=4),    # Crop alÃ©atoire
    transforms.ToTensor(),                   # Conversion en tenseur
    transforms.Normalize((0.4914, 0.4822, 0.4465), 
                         (0.2023, 0.1994, 0.2010))  # Normalisation
])
```

**RÃ©sultat** : DataLoaders configurÃ©s avec DistributedSampler

### 3ï¸âƒ£ Distributed Training

```python
# Configuration du training distribuÃ©
if world_size > 1:
    dist.init_process_group(backend='gloo', ...)
    model = DDP(model)

# EntraÃ®nement sur 10 epochs
for epoch in range(10):
    # Training loop avec synchronisation automatique
    ...
```

**StratÃ©gie** : 
- PyTorch Distributed Data Parallel (DDP)
- Backend Gloo (optimisÃ© CPU)
- Synchronisation des gradients automatique

### 4ï¸âƒ£ Model Evaluation

```python
# Ã‰valuation Ã  chaque epoch
model.eval()
with torch.no_grad():
    for inputs, targets in testloader:
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        # Calcul accuracy
        ...
```

**MÃ©triques calculÃ©es** :
- Train Loss & Accuracy
- Test Loss & Accuracy

### 5ï¸âƒ£ Model Versioning

```python
# Sauvegarde avec timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Checkpoint Ã  chaque epoch
torch.save(checkpoint, f'/output/checkpoint_epoch_{epoch+1}.pth')

# Upload vers MinIO
minio_client.fput_object(
    "models",
    f"resnet-cifar10/model_v{timestamp}.pth",
    final_model_path
)
```

**Organisation MinIO** :
```
models/
â””â”€â”€ resnet-cifar10/
    â”œâ”€â”€ checkpoint_epoch_1.pth
    â”œâ”€â”€ checkpoint_epoch_2.pth
    â”œâ”€â”€ ...
    â”œâ”€â”€ checkpoint_epoch_10.pth
    â”œâ”€â”€ model_v20241228_143022.pth
    â””â”€â”€ model_latest.pth

metrics/
â””â”€â”€ resnet-cifar10/
    â””â”€â”€ metrics_v20241228_143022.json
```

---

## ğŸ“ˆ RÃ©sultats

### MÃ©triques de Performance

**Configuration** :
- ModÃ¨le : ResNet-18 (11M paramÃ¨tres)
- Dataset : CIFAR-10 (60K images)
- Epochs : 10
- Batch Size : 128
- Learning Rate : 0.1 (avec cosine annealing)
- Optimizer : SGD (momentum=0.9, weight_decay=5e-4)
- Workers : 1 Master + 1 Worker

**RÃ©sultats Attendus** :

| Epoch | Train Loss | Train Acc | Test Loss | Test Acc |
|-------|------------|-----------|-----------|----------|
| 1 | 1.875 | 31.2% | 1.652 | 39.8% |
| 2 | 1.512 | 45.6% | 1.398 | 50.2% |
| 5 | 0.983 | 65.4% | 0.945 | 67.1% |
| 10 | 0.421 | 85.3% | 0.682 | 78.5% |

**Graphiques** :

```
Train vs Test Accuracy
90% â”¤                                    â•­â”€â”€
80% â”¤                           â•­â”€â”€â”€â”€â”€â”€â”€â”€â•¯
70% â”¤                    â•­â”€â”€â”€â”€â”€â”€â•¯
60% â”¤              â•­â”€â”€â”€â”€â”€â•¯
50% â”¤        â•­â”€â”€â”€â”€â”€â•¯
40% â”¤   â•­â”€â”€â”€â”€â•¯
30% â”¼â”€â”€â”€â•¯
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1   2   3   4   5   6   7   8   9   10
                    Epochs
```

### Temps d'ExÃ©cution

| Phase | DurÃ©e | Description |
|-------|-------|-------------|
| Setup | 2-3 min | DÃ©ploiement MinIO, crÃ©ation buckets |
| Data Download | 1-2 min | CIFAR-10 download (premiÃ¨re fois) |
| Training | 20-25 min | 10 epochs complets |
| Saving | 1-2 min | Upload vers MinIO |
| **Total** | **25-30 min** | Pipeline complet |

### Utilisation Ressources

```bash
# Pendant l'entraÃ®nement
kubectl top nodes
kubectl top pods -n mlops-light
```

**Ressources typiques** :
- Master : ~1.5 GB RAM, 80% CPU
- Worker : ~1.5 GB RAM, 80% CPU
- MinIO : ~250 MB RAM, 10% CPU

---

## ğŸ” Monitoring

### Surveiller l'EntraÃ®nement

#### 1. Status du PyTorchJob

```bash
kubectl get pytorchjob -n mlops-light
```

**Sortie** :
```
NAME           STATE       AGE
resnet-light   Running     15m
```

**Ã‰tats possibles** :
- `Created` : Job crÃ©Ã©, pods en cours de dÃ©marrage
- `Running` : EntraÃ®nement en cours
- `Succeeded` : EntraÃ®nement terminÃ© avec succÃ¨s
- `Failed` : Ã‰chec de l'entraÃ®nement

#### 2. Pods d'EntraÃ®nement

```bash
kubectl get pods -n mlops-light
```

**Sortie** :
```
NAME                      READY   STATUS    RESTARTS   AGE
minio-xxxxx               1/1     Running   0          20m
resnet-light-master-0     1/1     Running   0          18m
resnet-light-worker-0     1/1     Running   0          18m
```

#### 3. Logs en Temps RÃ©el

**Master (Rank 0)** :
```bash
kubectl logs -f -l training.kubeflow.org/job-name=resnet-light,training.kubeflow.org/replica-type=master -n mlops-light
```

**Worker (Rank 1)** :
```bash
kubectl logs -f -l training.kubeflow.org/job-name=resnet-light,training.kubeflow.org/replica-type=worker -n mlops-light
```

**Exemple de logs** :
```
============================================================
[Rank 0/2] ğŸš€ DÃ‰MARRAGE ENTRAÃNEMENT DISTRIBUÃ‰
============================================================

[Rank 0] ğŸ“¥ Ã‰TAPE 1/5: Data Ingestion
[Rank 0] âœ“ 50000 train, 10000 test

[Rank 0] ğŸ”§ Ã‰TAPE 2/5: Feature Engineering
[Rank 0] âœ“ DataLoaders configurÃ©s

[Rank 0] ğŸ—ï¸  Ã‰TAPE 3/5: Model Creation
[Rank 0] âœ“ ResNet-18 crÃ©Ã©

[Rank 0] ğŸ¯ EntraÃ®nement 10 epochs...

  Epoch 1/10 [0/391] Loss: 2.303 Acc: 9.38%
  Epoch 1/10 [50/391] Loss: 2.156 Acc: 18.75%
  ...
```

### AccÃ©der Ã  MinIO Console

```bash
# Port-forwarding
kubectl port-forward -n mlops-light svc/minio 9001:9001
```

Puis ouvrir dans le navigateur : **http://localhost:9001**

**Credentials** :
- Username : `minioadmin`
- Password : `minioadmin`

**Navigation** :
1. Cliquer sur "Buckets"
2. SÃ©lectionner `models`
3. Naviguer dans `resnet-cifar10/`
4. TÃ©lÃ©charger les checkpoints et mÃ©triques

### Visualiser les MÃ©triques

```bash
# TÃ©lÃ©charger les mÃ©triques
kubectl port-forward -n mlops-light svc/minio 9000:9000 &

# Script Python pour visualiser
python3 << 'EOF'
from minio import Minio
import json
import tempfile

client = Minio("localhost:9000", access_key="minioadmin", secret_key="minioadmin", secure=False)

# Liste des fichiers de mÃ©triques
objects = list(client.list_objects("metrics", prefix="resnet-cifar10/", recursive=True))

if objects:
    # TÃ©lÃ©charger le plus rÃ©cent
    latest = sorted(objects, key=lambda x: x.last_modified, reverse=True)[0]
    
    with tempfile.NamedTemporaryFile(mode='w+b', delete=False) as tmp:
        client.fget_object("metrics", latest.object_name, tmp.name)
        with open(tmp.name, 'r') as f:
            metrics = json.load(f)
    
    print("\n" + "="*60)
    print("           RÃ‰SULTATS FINAUX")
    print("="*60)
    print(f"\nğŸ“Š Epochs: {len(metrics['epochs'])}")
    print(f"\nğŸ¯ Accuracy Finale:")
    print(f"   Train: {metrics['train_acc'][-1]:.2f}%")
    print(f"   Test:  {metrics['test_acc'][-1]:.2f}%")
    print(f"\nğŸ“ˆ Meilleure Accuracy:")
    print(f"   Train: {max(metrics['train_acc']):.2f}%")
    print(f"   Test:  {max(metrics['test_acc']):.2f}%")
else:
    print("âŒ Aucune mÃ©trique trouvÃ©e")
EOF
```

---

## ğŸš€ DÃ©ploiement

### DÃ©ployer le ModÃ¨le pour InfÃ©rence

Une fois l'entraÃ®nement terminÃ©, dÃ©ployez une API REST pour faire des prÃ©dictions.

#### CrÃ©er le Service d'InfÃ©rence

```bash
nano deploy-inference.yaml
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-code
  namespace: mlops-light
data:
  serve.py: |
    from flask import Flask, request, jsonify
    import torch
    import torchvision
    import torchvision.transforms as transforms
    from PIL import Image
    import io
    from minio import Minio
    
    app = Flask(__name__)
    
    CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
    
    transform = transforms.Compose([
        transforms.Resize(32),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    def load_model():
        client = Minio("minio.mlops-light.svc.cluster.local:9000",
                      access_key="minioadmin", secret_key="minioadmin", secure=False)
        
        client.fget_object("models", "resnet-cifar10/model_latest.pth", "/tmp/model.pth")
        
        model = torchvision.models.resnet18(weights=None)
        model.fc = torch.nn.Linear(model.fc.in_features, 10)
        
        checkpoint = torch.load("/tmp/model.pth", map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        return model
    
    model = load_model()
    
    @app.route('/predict', methods=['POST'])
    def predict():
        file = request.files['file']
        img = Image.open(io.BytesIO(file.read())).convert('RGB')
        img_tensor = transform(img).unsqueeze(0)
        
        with torch.no_grad():
            outputs = model(img_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            conf, pred = probs.max(1)
        
        return jsonify({
            "prediction": CLASSES[pred.item()],
            "confidence": float(conf.item())
        })
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resnet-inference
  namespace: mlops-light
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference
  template:
    metadata:
      labels:
        app: inference
    spec:
      containers:
      - name: api
        image: python:3.9
        command:
        - bash
        - -c
        - |
          pip install flask torch torchvision minio pillow
          python /app/serve.py
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: code
          mountPath: /app
        resources:
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: code
        configMap:
          name: inference-code
---
apiVersion: v1
kind: Service
metadata:
  name: inference
  namespace: mlops-light
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    app: inference
```

#### DÃ©ployer

```bash
kubectl apply -f deploy-inference.yaml

# Attendre que le pod soit prÃªt
kubectl wait --for=condition=ready pod -l app=inference -n mlops-light --timeout=300s
```

#### Tester l'API

```bash
# Port-forward
kubectl port-forward -n mlops-light svc/inference 8080:8080 &

# Test avec une image
curl -X POST -F "file=@test_image.png" http://localhost:8080/predict
```

**RÃ©ponse attendue** :
```json
{
  "prediction": "cat",
  "confidence": 0.9234
}
```

---

## ğŸ› ï¸ Troubleshooting

### ProblÃ¨mes Courants

#### 1. Pods en `Pending`

**SymptÃ´me** :
```bash
kubectl get pods -n mlops-light
# resnet-light-master-0   0/1   Pending   0   5m
```

**Cause** : Ressources insuffisantes

**Solution** :
```bash
# Supprimer et recrÃ©er Minikube avec plus de mÃ©moire
minikube delete
minikube start --cpus=2 --memory=8192 --disk-size=20g --driver=docker
```

#### 2. `ImagePullBackOff`

**SymptÃ´me** :
```bash
kubectl describe pod -n mlops-light resnet-light-master-0
# Warning  Failed  ... Failed to pull image "pytorch/pytorch:2.0.0"
```

**Cause** : ProblÃ¨me rÃ©seau ou image inexistante

**Solution** :
```bash
# VÃ©rifier la connexion
docker pull pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime

# Si Ã§a Ã©choue, utiliser une image plus lÃ©gÃ¨re
# Ã‰diter lightweight-pipeline.yaml :
# image: pytorch/pytorch:2.0.0-cpu
```

#### 3. Pods en `CrashLoopBackOff`

**SymptÃ´me** :
```bash
kubectl get pods -n mlops-light
# resnet-light-master-0   0/1   CrashLoopBackOff   3   5m
```

**Solution** :
```bash
# Voir les logs d'erreur
kubectl logs -n mlops-light resnet-light-master-0

# ProblÃ¨mes courants :
# - Erreur Python â†’ VÃ©rifier le code dans le ConfigMap
# - Erreur MinIO â†’ VÃ©rifier que MinIO est Running
# - OOMKilled â†’ Augmenter les ressources mÃ©moire
```

#### 4. MinIO Inaccessible

**SymptÃ´me** :
```
âš ï¸  MinIO: connection refused
```

**Solution** :
```bash
# VÃ©rifier MinIO
kubectl get pods -n mlops-light -l app=minio

# RedÃ©marrer MinIO
kubectl rollout restart deployment minio -n mlops-light

# Attendre
kubectl wait --for=condition=ready pod -l app=minio -n mlops-light --timeout=120s
```

#### 5. Training Operator Non InstallÃ©

**SymptÃ´me** :
```
error: unable to recognize "lightweight-pipeline.yaml": 
no matches for kind "PyTorchJob"
```

**Solution** :
```bash
# VÃ©rifier l'installation
kubectl get pods -n kubeflow

# Si absent, rÃ©installer
cd ~/mlops-distributed-project/training-operator
kubectl apply -k manif